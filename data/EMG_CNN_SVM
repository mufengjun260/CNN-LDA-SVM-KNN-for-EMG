# coding=utf-8

import tensorflow as tf
import numpy as np
import os
import scipy.io as sio
import string
import math
from sklearn import svm
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier

path1 = '/home/taowucheng/Desktop/Pythonprojects/tensorflow/EMG/CNN/data/data/train/'
path2 = '/home/taowucheng/Desktop/Pythonprojects/tensorflow/EMG/CNN/data/data/test/'

right1 = 0.0
right2 = 0.0
right3 = 0.0

def LoadData(p1,p2):
    X_train_orig = np.zeros((40950,300,16,1))   #44850
    X_test_orig = np.zeros((1,40950),dtype=int)
    Y_train_orig = np.zeros((5850,300,16,1))   #5850
    Y_test_orig = np.zeros((1,5850),dtype=int)

    m = 0
    for files1 in os.listdir(p1):
        EMG = sio.loadmat(p1 + files1)
        data = EMG['data']
        for i in range(195):
            X_train_orig[m * 195 + i, :, :, 0] = data[i*50:i*50+300, :]
            X_test_orig[0, m * 195 + i] = string.atoi(files1[6:7])
        m = m + 1

    n = 0
    for files2 in os.listdir(p2):
        EMG = sio.loadmat(p2 + files2)
        data = EMG['data']
        for i in range(195):
            Y_train_orig[n * 195 + i, :, :, 0] = data[i*50:i*50+300, :]
            Y_test_orig[0, n * 195 + i] = string.atoi(files2[6:7])
        n = n + 1

    return X_train_orig,X_test_orig,Y_train_orig,Y_test_orig

def convert_to_one_hot(Y, C):
    Y = np.eye(C)[Y.reshape(-1)].T
    return Y

'''
def weight_variable(shape):
    # 用正态分布来初始化权值
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

def bias_variable(shape):
    # 本例中用relu激活函数，所以用一个很小的正偏置较好
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)
'''

def create_placeholders(n_H0,n_W0,n_C0,n_y):
    X=tf.placeholder('float',[None,n_H0,n_W0,n_C0])
    Y=tf.placeholder('float',[None,n_y])
    return X,Y

'''
def batch_normalization(bnin):
    batch_mean,batch_var=tf.nn.moments(bnin,[0,1,2],keep_dims=True)
    shift=tf.Variable(tf.zeros([batch_mean]))
    scale=tf.Variable(tf.ones([batch_mean]))
    BN_out=tf.nn.batch_normalization(bnin,batch_mean,batch_var,shift,scale,epsilon)
    return BN_out
'''

def random_mini_batches(X, Y, mini_batch_size=10):

    m = X.shape[0]  # number of training examples
    mini_batches = []

    # Step 1: Shuffle (X, Y)
    permutation = list(np.random.permutation(m))
    shuffled_X = X[permutation, :, :, :]
    shuffled_Y = Y[permutation, :]

    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.
    num_complete_minibatches = int(math.floor(m / mini_batch_size))  # number of mini batches of size mini_batch_size in your partitionning
    for k in range(0, num_complete_minibatches):
        mini_batch_X = shuffled_X[k * mini_batch_size: k * mini_batch_size + mini_batch_size, :, :, :]
        mini_batch_Y = shuffled_Y[k * mini_batch_size: k * mini_batch_size + mini_batch_size, :]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)

    # Handling the end case (last mini-batch < mini_batch_size)
    if m % mini_batch_size != 0:
        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size: m, :, :, :]
        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size: m, :]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)

    return mini_batches

def mini_batches(X, Y, mini_batch_size=10):

    m = X.shape[0]  # number of training examples
    batches = []

    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.
    num_complete_minibatches = int(math.floor(m / mini_batch_size))  # number of mini batches of size mini_batch_size in your partitionning
    for k in range(0, num_complete_minibatches):
        mini_batch_X = X[k * mini_batch_size: k * mini_batch_size + mini_batch_size, :, :, :]
        mini_batch_Y = Y[k * mini_batch_size: k * mini_batch_size + mini_batch_size, :]
        mini_batch = (mini_batch_X, mini_batch_Y)
        batches.append(mini_batch)

    # Handling the end case (last mini-batch < mini_batch_size)
    if m % mini_batch_size != 0:
        mini_batch_X = X[num_complete_minibatches * mini_batch_size: m, :, :, :]
        mini_batch_Y = Y[num_complete_minibatches * mini_batch_size: m, :]
        mini_batch = (mini_batch_X, mini_batch_Y)
        batches.append(mini_batch)

    return batches

def batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):
    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, iteration) # adding the iteration prevents from averaging across non-existing iterations
    bnepsilon = 1e-5
    if convolutional:
        mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])
    else:
        mean, variance = tf.nn.moments(Ylogits, [0])
    update_moving_averages = exp_moving_avg.apply([mean, variance])
    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)
    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)
    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)
    return Ybn, update_moving_averages

def no_batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):
    return Ylogits, tf.no_op()

def compatible_convolutional_noise_shape(Y):
    noiseshape = tf.shape(Y)
    noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])
    return noiseshape


g = tf.Graph()
with g.as_default():
    X_train_orig, X_test_orig, Y_train_orig, Y_test_orig = LoadData(path1,path2)
    X_train = X_train_orig
    Y_train = Y_train_orig
    X_test = convert_to_one_hot(X_test_orig,10).T
    Y_test = convert_to_one_hot(Y_test_orig,10).T

# variable learning rate
    lr = tf.placeholder(tf.float32)
# test flag for batch norm
    tst = tf.placeholder(tf.bool)
    iter = tf.placeholder(tf.int32)
# dropout probability
    pkeep = tf.placeholder(tf.float32)
    pkeep_conv = tf.placeholder(tf.float32)

    (m, n_H0, n_W0, n_C0) = X_train.shape
    n_y = X_test.shape[1]
    X, Y_ = create_placeholders(n_H0, n_W0, n_C0, n_y)

#1:卷积
    B0 = tf.Variable(tf.constant(0.1, tf.float32, [1]))
    X1, update_ema0 = batchnorm(X, tst, iter, B0)
    W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 64], stddev=0.1))
    B1 = tf.Variable(tf.constant(0.1, tf.float32, [64]))
    Y1l = tf.nn.conv2d(X1, W1, strides=[1, 1, 1, 1], padding='SAME')
    Y1bn, update_ema1 = batchnorm(Y1l, tst, iter, B1, convolutional=True)
    Y11 = tf.nn.relu(Y1bn)

    Y1 = tf.nn.max_pool(Y11, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

#2:卷积
    W2 = tf.Variable(tf.truncated_normal([3, 3, 64, 64], stddev=0.1))
    B2 = tf.Variable(tf.constant(0.1, tf.float32, [64]))
    Y2l = tf.nn.conv2d(Y1, W2, strides=[1, 1, 1, 1], padding='SAME')
    Y2bn, update_ema2 = batchnorm(Y2l, tst, iter, B2, convolutional=True)
    Y22 = tf.nn.relu(Y2bn)

    Y2 = tf.nn.max_pool(Y22, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

#3:本地连接层
    W3 = tf.Variable(tf.truncated_normal([1, 1, 64, 64], stddev=0.1))
    B3 = tf.Variable(tf.constant(0.1, tf.float32, [64]))
    Y3l = tf.nn.conv2d(Y2, W3, strides=[1, 1, 1, 1], padding='SAME')
    Y3bn, update_ema3 = batchnorm(Y3l, tst, iter, B3, convolutional=True)
    Y3 = tf.nn.relu(Y3bn)

#4:本地连接层
    W4 = tf.Variable(tf.truncated_normal([1, 1, 64, 64], stddev=0.1))
    B4 = tf.Variable(tf.constant(0.1, tf.float32, [64]))
    Y4l = tf.nn.conv2d(Y3, W4, strides=[1, 1, 1, 1], padding='SAME')
    Y4bn, update_ema4 = batchnorm(Y4l, tst, iter, B4, convolutional=True)
    Y4r = tf.nn.relu(Y4bn)
    Y4 = tf.nn.dropout(Y4r, pkeep_conv, compatible_convolutional_noise_shape(Y4r))

# reshape the output from the third convolution for the fully connected layer
    YY = tf.reshape(Y4, shape=[-1, 64*75*4])

#5:全连接层
    W5 = tf.Variable(tf.truncated_normal([64*75*4, 512], stddev=0.1))
    B5 = tf.Variable(tf.constant(0.1, tf.float32, [512]))
    Y5l = tf.matmul(YY, W5)
    Y5bn, update_ema5 = batchnorm(Y5l, tst, iter, B5)
    Y5r = tf.nn.relu(Y5bn)
    Y5 = tf.nn.dropout(Y5r, pkeep)

#6:全连接层
    W6 = tf.Variable(tf.truncated_normal([512, 256], stddev=0.1))
    B6 = tf.Variable(tf.constant(0.1, tf.float32, [256]))
    Y6l = tf.matmul(Y5, W6)
    Y6bn, update_ema6 = batchnorm(Y6l, tst, iter, B6)
    Y6r = tf.nn.relu(Y6bn)
    Y6 = tf.nn.dropout(Y6r, pkeep)

#7:全连接层
    W7 = tf.Variable(tf.truncated_normal([256, 128], stddev=0.1))
    B7 = tf.Variable(tf.constant(0.1, tf.float32, [128]))
    Y7l = tf.matmul(Y6, W7)
    Y7bn, update_ema7 = batchnorm(Y7l, tst, iter, B7)
    Y7 = tf.nn.relu(Y7bn)

    W8 = tf.Variable(tf.truncated_normal([128, 64], stddev=0.1))
    B8 = tf.Variable(tf.constant(0.1, tf.float32, [64]))
    Y8l = tf.matmul(Y7, W8)
    Y8bn, update_ema8 = batchnorm(Y8l, tst, iter, B8)
    Y8 = tf.nn.relu(Y8bn)

    W9 = tf.Variable(tf.truncated_normal([64, 32], stddev=0.1))
    B9 = tf.Variable(tf.constant(0.1, tf.float32, [32]))
    Y9l = tf.matmul(Y8, W9)
    Y9bn, update_ema9 = batchnorm(Y9l, tst, iter, B9)
    Y9 = tf.nn.relu(Y9bn)

    W10 = tf.Variable(tf.truncated_normal([32, 16], stddev=0.1))
    B10 = tf.Variable(tf.constant(0.1, tf.float32, [16]))
    Y10l = tf.matmul(Y9, W10)
    Y10bn, update_ema10 = batchnorm(Y10l, tst, iter, B10)
    Y10 = tf.nn.relu(Y10bn)

    getfeature = tf.reshape(Y10, [-1, 16], name='getfeature')

#8:输出
    W11 = tf.Variable(tf.truncated_normal([16, 10], stddev=0.1))
    B11 = tf.Variable(tf.constant(0.1, tf.float32, [10]))
    Ylogits = tf.matmul(Y10, W11) + B11

#    getfeature = tf.reshape(Ylogits, [-1, 6], name='getfeature')

    Y = tf.nn.softmax(Ylogits)

    update_ema = tf.group(update_ema0, update_ema1, update_ema2, update_ema3, update_ema4, update_ema5, update_ema6, update_ema7, update_ema8, update_ema9, update_ema10)

# 损失函数：cross_entropy
#    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits,labels=Y_))
#    regularization_loss = tf.reduce_mean(tf.square(W11))
#    hinge_loss = tf.reduce_mean(tf.square(tf.maximum(tf.zeros([25, 10]), 1 - Y_ * Ylogits)))
#    cross_entropy = regularization_loss + 0.9 * hinge_loss  #  penalty_parameter = 1

    cross_entropy = -tf.reduce_sum(Y_ * tf.log(Y))


# 预测准确结果统计
# 预测值中最大值（１）即分类结果，是否等于原始标签中的（１）的位置。argmax()取最大值所在的下标
    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

#accuracy_1 = tf.argmax(tf.bincount(tf.cast(correct_prediction, tf.float32)))

# 优化函数：AdamOptimizer
    train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)

'''
# 如果一次性来做测试的话，可能占用的显存会比较多，所以测试的时候也可以设置较小的batch来看准确率
test_acc_sum = tf.Variable(0.0)
batch_acc = tf.placeholder(tf.float32)
new_test_acc_sum = tf.add(test_acc_sum, batch_acc)
update = tf.assign(test_acc_sum, new_test_acc_sum)
'''

# 定义了变量必须要初始化，或者下面形式
#sess.run(tf.global_variables_initializer())
# 或者某个变量单独初始化 如：
# x.initializer.run()



with tf.Session(graph=g) as sess:
    # Run the initialization
    sess.run(tf.global_variables_initializer())

    for i in range(50):
        j=0
        acc=0
        batchs = random_mini_batches(X_train, X_test, 25)
        for batch in batchs:
            j+=1
        # Select a minibatch
            (batch_X, batch_Y) = batch
        # learning rate decay
#            max_learning_rate = 0.02
#            min_learning_rate = 0.0001
#            decay_speed = 2000
#            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-j / decay_speed)
            learning_rate = 0.0001

        # compute training values for visualisation
            if (i+1) % 1 == 0:
                a, c = sess.run([accuracy, cross_entropy],{X: batch_X, Y_: batch_Y, tst: False, pkeep: 0.5, pkeep_conv: 0.5})
                acc += a
                if j == 897:   # 44850/50   4875/25
                    acc=acc/j
                    print("training step: " + str(i+1) + " loss: " + str(c) + " acc: " + str(acc))

        # the backpropagation training step
            sess.run(train_step, {X: batch_X, Y_: batch_Y, lr: learning_rate, tst: False, pkeep: 0.5, pkeep_conv: 0.5})
            sess.run(update_ema, {X: batch_X, Y_: batch_Y, tst: False, iter: i, pkeep: 0.5, pkeep_conv: 0.5})

    step = 0
    num = 0
    batchs = mini_batches(Y_train, Y_test, 25)
    for batch in batchs:
        (batch_X, batch_Y) = batch

        step += 1
        test_acc = sess.run(accuracy, feed_dict={X: batch_X, Y_: batch_Y, tst: True, pkeep: 0.5, pkeep_conv: 0.5})

        num += test_acc
        if step % 5 == 0:
            print "testing step %d, test_acc %g" % (step, test_acc)

    print("test accuracy: " + str(num / step))

    x_temp = []
    batchs = mini_batches(X_train, X_test, 1)
    for batch in batchs:
        (batch_X, batch_Y) = batch

        feature = g.get_operation_by_name("getfeature").outputs[0]
        x_temp.append(sess.run(feature, feed_dict={X: batch_X, Y_: batch_Y, tst: True, pkeep: 0.5, pkeep_conv: 0.5})[0])

    x_temp1 = []
    batchs = mini_batches(Y_train, Y_test, 1)
    for batch in batchs:
        (batch_X, batch_Y) = batch

        feature = g.get_operation_by_name("getfeature").outputs[0]
        x_temp1.append(sess.run(feature, feed_dict={X: batch_X, Y_: batch_Y, tst: True, pkeep: 0.5, pkeep_conv: 0.5})[0])


    #print x_temp[1]
    #print x_temp[2]
    #print x_temp1[1]
    #eprint x_temp1[2]
    #print len(x_temp[1])
    #print len(x_temp1)
    #print len(x_temp1[1])


    X_test = np.reshape(X_test_orig, 40950)
    Y_test = np.reshape(Y_test_orig, 5850)

    #SVM分类
    clf = svm.SVC(C=0.9, kernel='rbf')
    clf.fit(x_temp, X_test.tolist())# SVM选择了rbf核，C选择了0.9
    output1 = clf.predict(x_temp1)

    #LDA分类
    clf1 = LinearDiscriminantAnalysis()
    clf1.fit(x_temp, X_test.tolist())
    output2 = clf1.predict(x_temp1)


    #KNN分类
    neigh = KNeighborsClassifier(n_neighbors=3)
    neigh.fit(x_temp, X_test.tolist())
    output3 = neigh.predict(x_temp1)


    #print (output.tolist())
    #print type(output)
    #print len(Y_test)
    #print type(Y_test)
    #print type(x_temp1)
    #print output.tolist()
    #print Y_test.tolist()

    for j in range(5850):
        if output1[j] == Y_test[j]:
            right1 += 1
        if output2[j] == Y_test[j]:
            right2 += 1
        if output3[j] == Y_test[j]:
            right3 += 1

    accuracy1 = right1 / 5850   # 准确率
    accuracy2 = right2 / 5850  # 准确率
    accuracy3 = right3 / 5850  # 准确率

    print("test accuracy (SVM): " + str(accuracy1))
    print("test accuracy (LDA): " + str(accuracy2))
    print("test accuracy (KNN): " + str(accuracy3))


'''
    # compute test values for visualisation
    step = 0
    test_num = 0
    batchs = mini_batches(Y_train, Y_test, 50)

    for batch in batchs:
        (batch_X, batch_Y) = batch
        #a, c = sess.run([accuracy, cross_entropy], {X: batch_X, Y_: batch_Y, tst: True, pkeep: 1.0, pkeep_conv: 1.0})
        step += 1
        test_acc = sess.run(accuracy, feed_dict={X: batch_X, Y_: batch_Y, tst: True, pkeep: 0.5, pkeep_conv: 0.5})
        if test_acc >= 0.5:
            test_acc = 1
            test_num += test_acc
        if test_acc < 0.5:
            test_acc = 0
        if step % 100 == 0:
            print "testing step %d, test_acc %g" % (step, test_num/step)

    print "test_accuracy: %g" % (test_num / 400.0)
'''

'''
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    # 训练
    minibatch_size = 10
    num_minibatches = int(m / minibatch_size)  # number of minibatches of size minibatch_size in the train set
    minibatches = random_mini_batches(X_train, X_test, minibatch_size)
    step = 0

    for minibatch in minibatches:
        step += 1
        # Select a minibatch
        (minibatch_X, minibatch_Y) = minibatch
        if step % 500 == 0:
            # IMPORTANT: The line that runs the graph on a minibatch.
            # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).
            train_accuracy = accuracy.eval(feed_dict={X_: minibatch_X, Y_: minibatch_Y, keep_prob: 0.5})
            print "step %d, training acc %g" % (step, train_accuracy)
        train_step.run(feed_dict={X_: minibatch_X, Y_: minibatch_Y})

    # 全部训练完了再做测试，batch_size=100

    i = 0
    minibatches = random_mini_batches(Y_train, Y_test, minibatch_size)
    for minibatch in minibatches:
        i += 1
        # Select a minibatch
        (minibatch_X, minibatch_Y) = minibatch
        # IMPORTANT: The line that runs the graph on a minibatch.
        # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).
        test_acc = accuracy.eval(feed_dict={X_: minibatch_X, Y_: minibatch_Y, keep_prob: 1.0})
        update.eval(feed_dict={batch_acc: test_acc})
        if i % 500 == 0:
            print "testing step %d, test_acc_sum %g" % (i, test_acc_sum.eval())
    print " test_accuracy %g" % (test_acc_sum.eval() / i)

'''

'''
with tf.Session() as sess:
    # Run the initialization
    sess.run(tf.global_variables_initializer())

    # Do the training loop
    epoch = 0
    minibatches = random_mini_batches(X_train, X_test, 100)

    #训练
    for minibatch in minibatches:
        # Select a minibatch
        (minibatch_X, minibatch_Y) = minibatch
        # IMPORTANT: The line that runs the graph on a minibatch.
        # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).
        sess.run(train_step, feed_dict={X: minibatch_X, Y_: minibatch_Y, keep_prob: 0.5})
        epoch += 1
        if epoch % 100 == 0:
            train_acc=sess.run(accuracy, feed_dict={X:minibatch_X, Y_: minibatch_Y, keep_prob: 0.5})
            print ("After %d training steps, validation accurary is: %g" % (epoch, train_acc))
    #测试
    test_accuracy = accuracy.eval({X: Y_train, Y_: Y_test, keep_prob: 1})
    print("After %d training steps, test accurary is: %g" % (epoch+1, test_accuracy))
'''
